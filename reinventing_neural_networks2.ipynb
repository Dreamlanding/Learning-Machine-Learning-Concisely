
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and boilerplate to make graphs look better\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "\n",
    "def setup_graph(title='', x_label='', y_label='', fig_size=None):\n",
    "    fig = plt.figure()\n",
    "    if fig_size != None:\n",
    "        fig.set_size_inches(fig_size[0], fig_size[1])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinventing neural networks 2\n",
    "\n",
    "In the previous notebook, we went through experiments that determined that the common neuron representation works pretty well for learning. That representation is characterized by:\n",
    "* Weighted inputs (which can be adjusted)\n",
    "* An activation threshold (which can be adjusted)\n",
    "* Outputs are \"all-or-nothing\" (in my case, I used `-1` or `1`).\n",
    "\n",
    "We tested them by generating random configurations of all the \"knobs\" at our disposal - for each neuron, the input weights and the activation threshold.\n",
    "\n",
    "Now, we want to adjust the knobs. So, how do we do that? The goal is to adjust the knobs so as to reduce error. We can do that layer by layer, which would be a step in the right direction. But the next question is, what is the \"right\" direction to step in to reduce error. I remember that the idea of gradient descent is to take the largest step in the direction that will most reduce error, but I don't remember intuitively how it works. Let's see if we can derive it though...\n",
    "\n",
    "A few thoughts:\n",
    "* A key lesson I realized from the previous notebook is that we don't want to step in the direction that minimizes the particular training sample at hand ONLY - or else, I would expect that we would hop around quite a bit. Can you learn from samples one-by-one serially? Or do you need to buffer them and consider several as a group?\n",
    "* Before, we were trying to minimize all the knobs simultaneously. If there are, say, 10 knobs, that is effectively trying to search through a 10-dimensional space (#curse-of-dimensionality). If we, instead, break the knobs up by the layer in the network, that would make the searching much less resource-intensive.\n",
    "* I wonder if an algorithm like MCMC could be used for searching in these N-dimensional spaces for optimizations?\n",
    "\n",
    "Let's start by taking another stab at how to represent and run neural networks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "\n",
    "def gen_network_weights(neurons_by_layer, init_rand_min=-5, init_rand_max=5):\n",
    "    neural_network_weights = []\n",
    "    for layer_index, num_neurons in enumerate(neurons_by_layer):\n",
    "        if layer_index == 0:\n",
    "            weights = np.array([1] * num_neurons)\n",
    "            \n",
    "        else:\n",
    "            num_inputs = neurons_by_layer[layer_index - 1]\n",
    "            weights = np.array([[random.uniform(init_rand_min, init_rand_max) for i in range(num_inputs)]\n",
    "                                for i in range(num_neurons)])\n",
    "\n",
    "        neural_network_weights.append(weights)\n",
    "    return neural_network_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 1]),\n",