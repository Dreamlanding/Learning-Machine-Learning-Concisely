
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and boilerplate to make graphs look better\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "\n",
    "def setup_graph(title='', x_label='', y_label='', fig_size=None):\n",
    "    fig = plt.figure()\n",
    "    if fig_size != None:\n",
    "        fig.set_size_inches(fig_size[0], fig_size[1])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinventing neural networks\n",
    "\n",
    "(Intentionally reinventing the wheel)\n",
    "\n",
    "I wanted to try to derive the ideas of machine learning for myself. I do have some knowledge (so it's not completely independent of course), but I want to try to derive a lot of the ideas myself.\n",
    "\n",
    "The first sample problem is to write a machine learning algorithm to learn the xor function.\n",
    "\n",
    "So here's the xor logic table:\n",
    "\n",
    "| x0 | x1 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 0 |\n",
    "\n",
    "So we have a 2 input function that returns 1 value - something like this:\n",
    "\n",
    "<img src=\"images/xor_network1.png\"  style=\"width: 50%; height: 50%\" />\n",
    "\n",
    "My initial thought is that we can convolute through some linear model, and adjust coefficients to get the right answer.\n",
    "\n",
    "So the equation would look something like this:\n",
    "\n",
    "$W0*x0 + W1*x1 = y$\n",
    "\n",
    "You could put this into matrix form and add a bias coefficient, in which case it would look like this:\n",
    "\n",
    "$Ax +b = y$ where A is the vector containing the 2 weights, x is a free variable to hold the features - x0 and x1, and y is the vector of labels) - so something like this:\n",
    "\n",
    "<img src=\"images/xor_matrix_form1.png\" style=\"width: 50%; height: 50%\" />\n",
    "\n",
    "Before trying to write an algorithm to optimize the values of W0 and W1, let's first make sure this model is powerful enough to work for the xor function. After thinking for a minute, I was able to determine that the values W0 = 1 and W2 = -1 would be good values, since if we plug those weights into the equation, $W0*x0 + W1*x1 = y$, we get $1*x0 + -1*x1$, and that should come out to the values of y (or at least, the value should be > 0.5 for (0, 1) and (1, 0), and < 0.5 for (0, 0) and (1, 1) ). And it turns out, that works:\n",
    "* for x0=0 and x1=1: `1*0 + -1*0 = 0`\n",
    "* for x0=0 and x1=1: `1*0 + -1*1 = -1`\n",
    "* for x0=0 and x1=1: `1*1 + -1*0 = 1`\n",
    "* for x0=0 and x1=1: `1*0 + -1*0 = 0`\n",
    "\n",
    "Shoot - when I first ran the numbers in my head, I was thinking the 2nd equation came out to 1, not -1. And actually, graphing it out, xor would look like this:\n",
    "\n",
    "<img src=\"images/xor_graph.png\"  style=\"width: 400px; height: 400px;\" />\n",
    "\n",
    "And as you can clearly see, that is not linearly separable. That is why, when I tried a linear classifier, it didn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]), array([0]), array([0]), array([0])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that I'm making 10x repetitions of the logic table.\n",
    "X = [[0, 0],\n",
    "     [0, 1],\n",
    "     [1, 0],\n",
    "     [1, 1]]*10\n",
    "y = [0, 1, 1, 0]*10\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "[clf.predict([sample]) for sample in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is wrong - presumably because a linear model simply isn't powerful enough, whereas something like RandomForest is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]), array([1]), array([1]), array([0])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "[clf.predict([sample]) for sample in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So RandomForest clearly works, but linear models do not. So we need to add some power to our model. How can we change it to be more powerful? Some ideas that come to mind:\n",
    "\n",
    "* Move from a linear to a polynomial model.\n",
    "* Add another layer to the network.\n",
    "\n",
    "Would these work? Is it possible that adding layers is similar in function to adding polynomials?\n",
    "\n",
    "Let's go with adding a layer...\n",
    "\n",
    "So the very simplest layer I could think of would be this:\n",
    "\n",
    "<img src=\"images/xor_network2.png\"  style=\"width: 50%; height: 50%;\" />\n",
    "(note: weights should be w0, w1, w2, and w3)\n",
    "\n",
    "But I know that typically, when I've seen pictures of neural networks, the inputs from one layer go to each of the nodes in the next layer - more like this:\n",
    "\n",
    "<img src=\"images/xor_network3.png\"  style=\"width: 50%; height: 50%;\" />\n",
    "(note: weights should be w0, w1, w2, and w3)\n",
    "\n",
    "I think the reason for this is that, in the prior network (without the cross in the middle), the 2 layers of weights are still only being added to the same input, which is really functionally the same as just having 1 layer. So I'm going to test out the network with the cross.\n",
    "\n",
    "So now we have 4 weights to play with. Let's see if I could imagine a solution where the weights are adjusted so that xor works... on second thought, that sounds like a good job for a program - let's do it that way.\n",
    "\n",
    "### Finding weights programmatically\n",
    "\n",
    "A few initial thoughts on writing a program to learn:\n",
    "* The inputs for the second layer are not the inputs, but the outputs of the previous layer.\n",
    "* We probably can't just keep 4 variables (one for each weight we are trying to tune), because we might optimize for the first sample, and then reoptimize for the second, and third, and so forth. One solution would be to generate a number of hypotheses for the weights, and keep them all in memory, and then find the one that minimizes error for all samples. Later, we can try to actually tweak the weights.\n",
    "\n",
    "So let's try to write this. First, some helper functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rand_weight(weight_min, weight_max):\n",
    "    \"\"\"Generates a uniformly random weight between weight_min and weight_max.\"\"\"\n",
    "    return random.random() * (weight_max - weight_min) + weight_min\n",
    "\n",
    "def gen_hypothesis(weight_min, weight_max, num_weights):\n",
    "    return tuple([rand_weight(weight_min, weight_max) for i in range(num_weights)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.706551715382767"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_weight(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.814122290019785,\n",
       " 1.4739675581934826,\n",
       " -4.116735204818674,\n",
       " -3.690855764792645)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_hypothesis(-5, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node functionality\n",
    "\n",
    "Next, let's define the functionality of each node. I know there's the concept of an [Activation function](https://en.wikipedia.org/wiki/Activation_function) in common neural network terminology. I suppose there are several ways a given node could function:\n",
    "* It could always output some discrete value (like 1 and 0, or -1 and 1).\n",
    "* It could output a continuous value bounded within some range like a probability (e.g. 0 to 1).\n",
    "* It could output an unbounded continuous value.\n",
    "\n",
    "For starters, I'm going to just go with what seems the simplest - it's just going to output an unbounded continuous value taking by multiplying the input by the weight. So the equation to determine the output of a given node will be: $node(x) = w0 * x$.\n",
    "\n",
    "On second thought, though... now that we have a hidden layer, those nodes will have more than one input. So how should that be handled? It could be handled a few ways:\n",
    "* We could make each input have its own weight.\n",
    "* We could use the same weight for all of the inputs.\n",
    "\n",
    "I'm again going to choose the dumbest easiest approach and say each node only has a single weight (even though, if I recall correctly, neural networks typically have different weights for each input - but we'll get there eventually). And if there is more than 1 sample, we'll just add them all together.\n",
    "\n",
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, weight):\n",
    "        self.weight = weight\n",
    "    def run(self, node_input):\n",
    "        if type(node_input) != list:\n",
    "            node_input = [node_input]\n",
    "        return sum([self.weight * input_val for input_val in node_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the node implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 50, 60]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Node(10)\n",
    "[n.run([1]), n.run([5]), n.run([1, 2, 3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's try to write the network.\n",
    "\n",
    "Actually, just trying to write this, I immediately realize that we need to do some sort of convolution at the output node (to join its 2 inputs), so let's just add a 5th weight for that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_1(weights, sample):\n",
    "    input_layer = [Node(weights[0]), Node(weights[1])]\n",
    "    hidden_layer = [Node(weights[3]), Node(weights[2])]\n",
    "    output_layer = Node(weights[4])\n",
    "    \n",
    "    input_layer_output = [input_layer[i].run(sample[i]) for i in range(len(input_layer))]\n",
    "    hidden_layer_output = [hidden_layer[i].run(input_layer_output) for i in range(len(hidden_layer))]\n",
    "    output_layer_output = output_layer.run(hidden_layer_output)\n",
    "    \n",
    "    return output_layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net_1([0, 0, 0, 0, 0], [1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net_1([1, 1, 1, 1, 1], [1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,